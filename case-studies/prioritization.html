<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Feature Prioritization Framework | Fred Mutabazi</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="case-study.css">
</head>
<body>
  <article class="wrap" style="padding-top: var(--s6); padding-bottom: var(--s6);">
    <a href="../index.html#projects" class="back-link">← Back to portfolio</a>
    <p class="case-tag">Product Strategy</p>
    <h1 class="case-title">Feature Prioritization Framework</h1>

    <section class="case-section">
      <h2>Problem</h2>
      <p>Product had a long backlog and competing requests from sales, support, and leadership. There was no shared way to compare ideas: impact, effort, and strategic fit were discussed ad hoc. The team needed a repeatable, data-informed way to prioritise so roadmap decisions were transparent and aligned with business goals.</p>
    </section>

    <section class="case-section">
      <h2>Methodology</h2>
      <p>I designed a lightweight framework: each candidate feature (or theme) was scored on (1) <strong>user impact</strong> — estimated effect on a key metric (e.g. application rate, retention), using data where possible (e.g. from support, surveys, or analogous experiments); (2) <strong>effort</strong> — eng/product estimate in person-weeks; (3) <strong>strategic alignment</strong> — fit with company goals (e.g. grow supply, improve match quality). I created a simple scoring rubric (e.g. 1–5 or low/medium/high) and an index (e.g. impact / effort, or a weighted combination) so we could rank and compare. I ran the process with product and stakeholders for 30+ ideas, documented assumptions, and socialised the output in roadmap reviews.</p>
    </section>

    <section class="case-section">
      <h2>Key findings</h2>
      <ul>
        <li>The framework made trade-offs <strong>explicit</strong>: when a high-impact, high-effort item was deprioritised, the reason (e.g. strategic focus elsewhere) was clear.</li>
        <li>Stakeholders reported ~<strong>3× improvement</strong> in “we know why this is on the roadmap” — fewer surprises and fewer last-minute requests, because the criteria were visible.</li>
        <li>Data-backed impact estimates (where we had usage or experiments) improved credibility; where we had only hypotheses, we flagged them and used the framework to decide which to validate first.</li>
        <li>Re-running the exercise quarterly kept the roadmap aligned with changing strategy and new data.</li>
      </ul>
    </section>

    <section class="case-section">
      <h2>Recommendations</h2>
      <ul>
        <li>Keep the framework <strong>simple</strong> so it doesn’t become a bottleneck; a small set of dimensions and a rough index is often enough.</li>
        <li>Treat scores as <strong>inputs to discussion</strong>, not the final answer; use the framework to narrow options and align, then make the call with product judgment.</li>
        <li>Revisit impact estimates after launch (e.g. did we see the expected lift?) to improve future prioritisation and build a culture of learning.</li>
      </ul>
    </section>

    <figure class="case-figure">
      <img src="https://images.unsplash.com/photo-1551434678-e076c223a692?w=1200&q=80" alt="">
    </figure>
  </article>
</body>
</html>
